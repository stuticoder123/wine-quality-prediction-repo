import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings('ignore')

# Load dataset
wine_df = pd.read_csv('WineQT.csv')

# Basic exploration
print(wine_df.head())
print(wine_df.tail())
print(wine_df.sample(7))
print(wine_df.columns)
print(wine_df.shape)
wine_df.info()

# Statistical summary
print(wine_df.describe())

# Check missing values
print(wine_df.isnull().sum())

# ---------------- Visualization ---------------- #

sns.set(style="whitegrid")

# Count of wine quality
print(wine_df['quality'].value_counts())
plt.figure(figsize=(10,6))
sns.countplot(x='quality', data=wine_df, palette='pastel')
plt.show()

# Correlation heatmap
plt.figure(figsize=(10,8))
sns.heatmap(wine_df.corr(), annot=True, cmap='PuBuGn')
plt.show()

# Distribution plots (before transformation)
fig, ax = plt.subplots(3, 4, figsize=(20, 30))
columns = wine_df.columns
k = 0

for i in range(3):
    for j in range(4):
        sns.histplot(wine_df[columns[k]], ax=ax[i][j], kde=True, color='red')
        k += 1

plt.show()

# ---------------- Log Transformation ---------------- #

def log_transform(col):
    return np.log(col[0] + 1)   # +1 to avoid log(0)

log_cols = [
    'residual sugar',
    'chlorides',
    'free sulfur dioxide',
    'total sulfur dioxide',
    'sulphates'
]

for col in log_cols:
    wine_df[col] = wine_df[[col]].apply(log_transform, axis=1)

# Distribution plots (after transformation)
fig, ax = plt.subplots(3, 4, figsize=(24, 30))
columns = wine_df.columns
k = 0

for i in range(3):
    for j in range(4):
        sns.histplot(wine_df[columns[k]], ax=ax[i][j], kde=True, color='green')
        k += 1

plt.show()

# Correlation with target
print(wine_df.corr()['quality'].sort_values(ascending=False))

# ---------------- Model Building ---------------- #

X = wine_df.drop(columns=['quality'])
y = wine_df['quality']

print(y.value_counts())

# Handle imbalance using SMOTE
from imblearn.over_sampling import SMOTE
oversample = SMOTE(k_neighbors=4, random_state=42)
X, y = oversample.fit_resample(X.fillna(0), y)

print(y.value_counts())

# Train-test & evaluation function
from sklearn.model_selection import train_test_split

def classify(model, X, y):
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42
    )
    model.fit(X_train, y_train)
    return model.score(X_test, y_test) * 100

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

LinearReg_acc = classify(LogisticRegression(max_iter=1000), X, y)
DecTree_acc = classify(DecisionTreeClassifier(), X, y)
RanFor_acc = classify(RandomForestClassifier(), X, y)
SVM_acc = classify(SVC(kernel='rbf'), X, y)

print("Logistic Regression Accuracy:", LinearReg_acc)
print("Decision Tree Accuracy:", DecTree_acc)
print("Random Forest Accuracy:", RanFor_acc)
print("SVM Accuracy:", SVM_acc)

# ---------------- Accuracy Comparison ---------------- #

Accuracy = [LinearReg_acc, DecTree_acc, RanFor_acc, SVM_acc]
models = [
    'Logistic Regression',
    'Decision Tree',
    'Random Forest',
    'Support Vector Machine'
]

plt.figure(figsize=(10,6))
sns.barplot(x=Accuracy, y=models)
plt.xlabel('Accuracy (%)')
plt.title('Model Accuracy Comparison')
plt.show()

